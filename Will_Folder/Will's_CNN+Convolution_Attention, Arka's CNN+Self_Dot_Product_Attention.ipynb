{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdE68v8n5SjV"
   },
   "outputs": [],
   "source": [
    "##code taken from \"https://github.com/Escanor1996/Speech-Emotion-Recognition-SER-/blob/master/SER.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qr7qgkA45eXv"
   },
   "source": [
    "### Loading the Header Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 5142,
     "status": "ok",
     "timestamp": 1699660973534,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "FYQg_waI5C_Y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backend_bases import RendererBase\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "#import soundfile as sf\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.fftpack import fft\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.onnx\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1699661053536,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "-DZzfsScbZTr"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47abuQ95914S"
   },
   "source": [
    "##### Function to Create the List of Files Directory/ store the Files in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1699661056014,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "zluuq5T5be_f"
   },
   "outputs": [],
   "source": [
    "def file_search(dirname, ret, audio_file, list_avoid_dir=[]):\n",
    "    filenames = os.listdir(dirname)\n",
    "\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        audio_name = filename\n",
    "\n",
    "        if os.path.isdir(full_filename) :\n",
    "            if full_filename.split('/')[-1] in list_avoid_dir:\n",
    "                continue\n",
    "            else:\n",
    "                file_search(full_filename, ret, list_avoid_dir)\n",
    "\n",
    "        else:\n",
    "            ret.append( full_filename )\n",
    "            audio_file.append(audio_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rest of the codes are taken from 'https://github.com/aris-ai/Audio-and-text-based-emotion-recognition/blob/master/GitAudioEmotion.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuKEtXaTeZmC"
   },
   "source": [
    "### Storing the Name of the Files in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18554,
     "status": "ok",
     "timestamp": 1699661079482,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "_e1Yjps3cBVp",
    "outputId": "4d748d4f-ed73-4d21-bc53-85f5ba2e443a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor_01, #sum files: 60\n",
      "Actor_02, #sum files: 120\n",
      "Actor_03, #sum files: 180\n",
      "Actor_04, #sum files: 240\n",
      "Actor_05, #sum files: 300\n",
      "Actor_06, #sum files: 360\n",
      "Actor_07, #sum files: 420\n",
      "Actor_08, #sum files: 480\n",
      "Actor_09, #sum files: 540\n",
      "Actor_10, #sum files: 600\n",
      "Actor_11, #sum files: 660\n",
      "Actor_12, #sum files: 720\n",
      "Actor_13, #sum files: 780\n",
      "Actor_14, #sum files: 840\n",
      "Actor_15, #sum files: 900\n",
      "Actor_16, #sum files: 960\n",
      "Actor_17, #sum files: 1020\n",
      "Actor_18, #sum files: 1080\n",
      "Actor_19, #sum files: 1140\n",
      "Actor_20, #sum files: 1200\n",
      "Actor_21, #sum files: 1260\n",
      "Actor_22, #sum files: 1320\n",
      "Actor_23, #sum files: 1380\n",
      "Actor_24, #sum files: 1440\n"
     ]
    }
   ],
   "source": [
    "list_files = []\n",
    "\n",
    "##storing only the names of the audio file\n",
    "audio_file = []\n",
    "for x in range(24):\n",
    "\n",
    "    ##converting the iteration to string\n",
    "    check_iter = str(x)\n",
    "\n",
    "    if x == 9:\n",
    "      sess_name = 'Actor_' + str(x+1)\n",
    "    elif len(check_iter) == 1:\n",
    "        sess_name = 'Actor_0' + str(x+1)\n",
    "    else:\n",
    "        sess_name = 'Actor_' + str(x+1)\n",
    "    #path = r'9444_WeightDancers_projarchive//'+ sess_name + '//'\n",
    "    path = r\"C:/Users/User/9444_WeightDancers_proj/archive/\" + sess_name + '/'\n",
    "    file_search(path, list_files, audio_file)\n",
    "    list_files = sorted(list_files)\n",
    "    print (sess_name + \", #sum files: \" + str(len(list_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1699661099372,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "-6HAbqh98pXb",
    "outputId": "e18d9720-ecf4-4a61-cf70-2f00f47cba84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_files)###total number of files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699661102379,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "a520GGh_gFZP",
    "outputId": "dd968bb2-f085-43fd-b22f-580ae0524cf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-01-01-01.wav'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files[0]##getting the first element of the file in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZaDWvFB9Twv"
   },
   "source": [
    "### Making the Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1699661107126,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "sdAiw6Yswa9u"
   },
   "outputs": [],
   "source": [
    "emotion_class = []\n",
    "for everyfile in list_files:\n",
    "  if (everyfile.split('/')[-1].strip('.wav')):\n",
    "    ##getting the actual audio file name\n",
    "    filename = everyfile.split('/')[-1].strip('.wav')\n",
    "\n",
    "    audio_file = filename.split('-')\n",
    "\n",
    "    ##getting the emotion class\n",
    "    label = audio_file[2]\n",
    "\n",
    "    ##making the class labels as either '0', '1', '2', '3', '4', '5', '6', '7'\n",
    "    number_label = int(label) - 1\n",
    "\n",
    "    emotion_class.append(number_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699661109243,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "BH0Am-7yzNtt",
    "outputId": "ca0e2ba7-f7fd-4be7-a0ea-0f70aa891eab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_class###printing of the 'emotion_class' labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd5-7Fqv-ZiT"
   },
   "source": [
    "### Making a Dataframe consisting of the Filenames along with the Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1699661113710,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "e4Z4v4QKzSIH"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for filenames, labels in zip(list_files, emotion_class):\n",
    "  data.append({\"path\": filenames, \"emotion_id\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699661115529,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "Be6kyQEA0MhT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_dataframe = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wdg07pS0hGl"
   },
   "source": [
    "#### Making the Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699661117546,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "cZC7oqIl0V7i",
    "outputId": "99e0e19c-08cc-460a-ac08-c18af582989a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>emotion_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-01-01-01.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-01-02-01.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-02-01-01.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-02-02-01.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-02-01-01-01-01.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-01-02-02-24.wav</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-02-01-01-24.wav</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-02-01-02-24.wav</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-02-02-01-24.wav</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-02-02-02-24.wav</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 path  \\\n",
       "0     C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-01-01-01.wav   \n",
       "1     C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-01-02-01.wav   \n",
       "2     C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-02-01-01.wav   \n",
       "3     C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-01-01-02-02-01.wav   \n",
       "4     C:/Users/User/9444_WeightDancers_proj/archive/Actor_01/03-01-02-01-01-01-01.wav   \n",
       "...                                                                               ...   \n",
       "1435  C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-01-02-02-24.wav   \n",
       "1436  C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-02-01-01-24.wav   \n",
       "1437  C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-02-01-02-24.wav   \n",
       "1438  C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-02-02-01-24.wav   \n",
       "1439  C:/Users/User/9444_WeightDancers_proj/archive/Actor_24/03-01-08-02-02-02-24.wav   \n",
       "\n",
       "      emotion_id  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "...          ...  \n",
       "1435           7  \n",
       "1436           7  \n",
       "1437           7  \n",
       "1438           7  \n",
       "1439           7  \n",
       "\n",
       "[1440 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest part of the code followed from 'https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqdFIZQt00kV"
   },
   "source": [
    "#### Reading audio from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 1706,
     "status": "ok",
     "timestamp": 1699661124435,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "bXXA8x9T0mO3"
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrXelLOHCEXZ"
   },
   "source": [
    "##### Preprocessing Functions are put under AudioUtil Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1699661128244,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "uw4chaiI090E"
   },
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "  # ----------------------------\n",
    "  # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def open(audio_file):\n",
    "    sig, sr = torchaudio.load(audio_file)\n",
    "    return (sig, sr)\n",
    "\n",
    "\n",
    "\n",
    "  # ----------------------------\n",
    "  # Convert the given audio to the desired number of channels\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    if (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    return ((resig, sr))\n",
    "\n",
    "  ##same sample rates for each audio signal\n",
    "  @staticmethod\n",
    "  def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "\n",
    "  ##resizing signals to the same length\n",
    "  @staticmethod\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "\n",
    "    return (sig, sr)\n",
    "\n",
    "  ###performing data augmentation by time shifting of audio signals\n",
    "  @staticmethod\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "  # ----------------------------\n",
    "  # Generate a Spectrogram\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=40, hop_len=20):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utxV51xrCO7b"
   },
   "source": [
    "#### Loading the Header Files for Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1699661133362,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "Q_5GbhUD1E20"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1699661134975,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "vk-QtiVw2h0N"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "    self.duration = 4000\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "\n",
    "  # ----------------------------\n",
    "  # Number of items in dataset\n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Absolute file path of the audio file - concatenate the audio directory with\n",
    "    # the relative path\n",
    "    audio_file = self.df.loc[idx, 'path']\n",
    "    # Get the Class ID/ Emotion ID\n",
    "    class_id = self.df.loc[idx, 'emotion_id']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "    # majority. So make all sounds have the same number of channels and same\n",
    "    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "    # result in arrays of different lengths, even though the sound duration is\n",
    "    # the same.\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    #aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1699661138122,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "HQIPQNkA4Czn"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "myds = SoundDS(final_dataframe, list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699661139342,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "VYy3pvxs4OZg",
    "outputId": "943d4a37-8307-4c4b-f2be-bf1b2479624c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1699661141464,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "7a7RWCnT4V1-"
   },
   "outputs": [],
   "source": [
    "num_items = len(myds)\n",
    "\n",
    "##Number of items in the Training Dataset\n",
    "num_train = round(num_items * 0.8)\n",
    "\n",
    "##Number of items in the Validation Dataset\n",
    "num_val = num_items - num_train\n",
    "\n",
    "##randomly splitting the data between training and valaidation dataset\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699661142978,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "yCpRzrO04ZRV"
   },
   "outputs": [],
   "source": [
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699661144662,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "DPsH3Wo94oRF",
    "outputId": "23b407d9-9b9f-46d5-a9f0-7451cde8988f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x22bedf93610>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699661146446,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "bb9sP8Eh5Nmu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6kzNV38G7bb"
   },
   "source": [
    "### Taking Self dot product Attention Model from \"https://spotintelligence.com/2023/01/31/self-attention/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "hOXumS60G5De"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpaYZvCPHgzK",
    "outputId": "8b034370-6d37-4074-e74b-add7f93be883"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        #self.lin = nn.Linear(in_features=64, out_features=8)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        #self.conv = nn.Sequential(*conv_layers)\n",
    "        self.attention_dot_self = SelfAttention(input_dim=64)\n",
    "\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Linear(in_features=64, out_features=16),\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Sequential(nn.Linear(in_features = 16, \n",
    "                                                    out_features = 8))\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        #x = x.view(x.shape[0], -1)\n",
    "        x = torch.flatten(x, start_dim=2)\n",
    "\n",
    "        # Linear layer\n",
    "        #x = self.lin(x)\n",
    "        #context_out = self.attention_dot_self(x.permute(0,2,1),x.permute(0,2,1))\n",
    "        context_out = self.attention_dot_self(x.permute(0, 2, 1))\n",
    "        \n",
    "        feature_map = context_out\n",
    "        \n",
    "        first_out_linear = self.hidden_layer(context_out.squeeze(1))\n",
    "        final_output = self.output_layer(first_out_linear)\n",
    "\n",
    "        # Final output\n",
    "        return final_output, feature_map\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25412\\2362672597.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tens = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.08, Accuracy: 0.17\n",
      "Epoch: 1, Loss: 2.02, Accuracy: 0.21\n",
      "Epoch: 2, Loss: 1.96, Accuracy: 0.23\n",
      "Epoch: 3, Loss: 1.91, Accuracy: 0.26\n",
      "Epoch: 4, Loss: 1.88, Accuracy: 0.27\n",
      "Epoch: 5, Loss: 1.85, Accuracy: 0.27\n",
      "Epoch: 6, Loss: 1.81, Accuracy: 0.28\n",
      "Epoch: 7, Loss: 1.79, Accuracy: 0.29\n",
      "Epoch: 8, Loss: 1.75, Accuracy: 0.30\n",
      "Epoch: 9, Loss: 1.77, Accuracy: 0.32\n",
      "Epoch: 10, Loss: 1.72, Accuracy: 0.31\n",
      "Epoch: 11, Loss: 1.71, Accuracy: 0.32\n",
      "Epoch: 12, Loss: 1.70, Accuracy: 0.34\n",
      "Epoch: 13, Loss: 1.65, Accuracy: 0.36\n",
      "Epoch: 14, Loss: 1.63, Accuracy: 0.38\n",
      "Epoch: 15, Loss: 1.57, Accuracy: 0.39\n",
      "Epoch: 16, Loss: 1.56, Accuracy: 0.41\n",
      "Epoch: 17, Loss: 1.52, Accuracy: 0.42\n",
      "Epoch: 18, Loss: 1.52, Accuracy: 0.42\n",
      "Epoch: 19, Loss: 1.49, Accuracy: 0.44\n",
      "Epoch: 20, Loss: 1.42, Accuracy: 0.45\n",
      "Epoch: 21, Loss: 1.40, Accuracy: 0.46\n",
      "Epoch: 22, Loss: 1.38, Accuracy: 0.47\n",
      "Epoch: 23, Loss: 1.38, Accuracy: 0.49\n",
      "Epoch: 24, Loss: 1.34, Accuracy: 0.49\n",
      "Epoch: 25, Loss: 1.28, Accuracy: 0.51\n",
      "Epoch: 26, Loss: 1.30, Accuracy: 0.50\n",
      "Epoch: 27, Loss: 1.28, Accuracy: 0.52\n",
      "Epoch: 28, Loss: 1.24, Accuracy: 0.51\n",
      "Epoch: 29, Loss: 1.26, Accuracy: 0.53\n",
      "Epoch: 30, Loss: 1.25, Accuracy: 0.54\n",
      "Epoch: 31, Loss: 1.23, Accuracy: 0.54\n",
      "Epoch: 32, Loss: 1.16, Accuracy: 0.55\n",
      "Epoch: 33, Loss: 1.16, Accuracy: 0.57\n",
      "Epoch: 34, Loss: 1.10, Accuracy: 0.60\n",
      "Epoch: 35, Loss: 1.11, Accuracy: 0.58\n",
      "Epoch: 36, Loss: 1.10, Accuracy: 0.58\n",
      "Epoch: 37, Loss: 1.05, Accuracy: 0.62\n",
      "Epoch: 38, Loss: 1.04, Accuracy: 0.63\n",
      "Epoch: 39, Loss: 0.96, Accuracy: 0.66\n",
      "Epoch: 40, Loss: 1.04, Accuracy: 0.63\n",
      "Epoch: 41, Loss: 0.99, Accuracy: 0.64\n",
      "Epoch: 42, Loss: 0.94, Accuracy: 0.66\n",
      "Epoch: 43, Loss: 0.95, Accuracy: 0.65\n",
      "Epoch: 44, Loss: 0.88, Accuracy: 0.68\n",
      "Epoch: 45, Loss: 0.84, Accuracy: 0.70\n",
      "Epoch: 46, Loss: 0.86, Accuracy: 0.68\n",
      "Epoch: 47, Loss: 0.80, Accuracy: 0.73\n",
      "Epoch: 48, Loss: 0.79, Accuracy: 0.73\n",
      "Epoch: 49, Loss: 0.79, Accuracy: 0.72\n",
      "Epoch: 50, Loss: 0.75, Accuracy: 0.73\n",
      "Epoch: 51, Loss: 0.73, Accuracy: 0.73\n",
      "Epoch: 52, Loss: 0.72, Accuracy: 0.75\n",
      "Epoch: 53, Loss: 0.67, Accuracy: 0.76\n",
      "Epoch: 54, Loss: 0.69, Accuracy: 0.75\n",
      "Epoch: 55, Loss: 0.64, Accuracy: 0.77\n",
      "Epoch: 56, Loss: 0.54, Accuracy: 0.81\n",
      "Epoch: 57, Loss: 0.59, Accuracy: 0.80\n",
      "Epoch: 58, Loss: 0.58, Accuracy: 0.80\n",
      "Epoch: 59, Loss: 0.54, Accuracy: 0.81\n",
      "Epoch: 60, Loss: 0.50, Accuracy: 0.82\n",
      "Epoch: 61, Loss: 0.51, Accuracy: 0.83\n",
      "Epoch: 62, Loss: 0.54, Accuracy: 0.81\n",
      "Epoch: 63, Loss: 0.47, Accuracy: 0.82\n",
      "Epoch: 64, Loss: 0.47, Accuracy: 0.83\n",
      "Epoch: 65, Loss: 0.45, Accuracy: 0.84\n",
      "Epoch: 66, Loss: 0.43, Accuracy: 0.86\n",
      "Epoch: 67, Loss: 0.44, Accuracy: 0.85\n",
      "Epoch: 68, Loss: 0.42, Accuracy: 0.85\n",
      "Epoch: 69, Loss: 0.40, Accuracy: 0.87\n",
      "Epoch: 70, Loss: 0.36, Accuracy: 0.89\n",
      "Epoch: 71, Loss: 0.34, Accuracy: 0.89\n",
      "Epoch: 72, Loss: 0.42, Accuracy: 0.85\n",
      "Epoch: 73, Loss: 0.37, Accuracy: 0.87\n",
      "Epoch: 74, Loss: 0.31, Accuracy: 0.90\n",
      "Epoch: 75, Loss: 0.35, Accuracy: 0.88\n",
      "Epoch: 76, Loss: 0.33, Accuracy: 0.88\n",
      "Epoch: 77, Loss: 0.32, Accuracy: 0.89\n",
      "Epoch: 78, Loss: 0.36, Accuracy: 0.87\n",
      "Epoch: 79, Loss: 0.31, Accuracy: 0.90\n",
      "Epoch: 80, Loss: 0.30, Accuracy: 0.90\n",
      "Epoch: 81, Loss: 0.27, Accuracy: 0.92\n",
      "Epoch: 82, Loss: 0.29, Accuracy: 0.90\n",
      "Epoch: 83, Loss: 0.30, Accuracy: 0.90\n",
      "Epoch: 84, Loss: 0.28, Accuracy: 0.91\n",
      "Epoch: 85, Loss: 0.27, Accuracy: 0.92\n",
      "Epoch: 86, Loss: 0.24, Accuracy: 0.92\n",
      "Epoch: 87, Loss: 0.25, Accuracy: 0.92\n",
      "Epoch: 88, Loss: 0.20, Accuracy: 0.94\n",
      "Epoch: 89, Loss: 0.24, Accuracy: 0.92\n",
      "Epoch: 90, Loss: 0.25, Accuracy: 0.91\n",
      "Epoch: 91, Loss: 0.22, Accuracy: 0.93\n",
      "Epoch: 92, Loss: 0.22, Accuracy: 0.93\n",
      "Epoch: 93, Loss: 0.21, Accuracy: 0.93\n",
      "Epoch: 94, Loss: 0.20, Accuracy: 0.94\n",
      "Epoch: 95, Loss: 0.22, Accuracy: 0.92\n",
      "Epoch: 96, Loss: 0.23, Accuracy: 0.93\n",
      "Epoch: 97, Loss: 0.20, Accuracy: 0.93\n",
      "Epoch: 98, Loss: 0.21, Accuracy: 0.93\n",
      "Epoch: 99, Loss: 0.19, Accuracy: 0.94\n",
      "Stopping early at epoch 99\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "  flag = 0\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,anneal_strategy='linear')\n",
    "  early_stop_thresh = 10\n",
    "  best_accuracy = -1\n",
    "  best_epoch = -1\n",
    "  map_list = []\n",
    "  labels_collection_map = []\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        #print(data)\n",
    "        #inputs, labels = data[0], torch.tensor(data[1])\n",
    "\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, atten_map = model(inputs)\n",
    "\n",
    "        #print(outputs)\n",
    "\n",
    "\n",
    "\n",
    "        labels_tens = torch.tensor(labels)\n",
    "\n",
    "        #print(labels_tens)\n",
    "\n",
    "        loss = criterion(outputs, labels_tens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "\n",
    "        ##appending the attention maps of the last iteration\n",
    "        if epoch - best_epoch > early_stop_thresh:\n",
    "            map_list.append(atten_map)\n",
    "            labels_collection_map.append(labels_tens)\n",
    "\n",
    "\n",
    "\n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "      best_accuracy = acc\n",
    "      best_epoch = epoch\n",
    "      checkpoint(myModel, 'best_model_CNN_Self_Dot.pth')\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "      print(f'Stopping early at epoch {epoch}')\n",
    "      flag = 1\n",
    "      #print(map_list)\n",
    "      break\n",
    "    else:\n",
    "      checkpoint(myModel, 'last_model_Self_Dot.pth')\n",
    "\n",
    "  if flag == 1:\n",
    "    return map_list, labels_collection_map\n",
    "\n",
    "num_epochs=150  # Just for demo, adjust this higher.\n",
    "atten_map, labels_tens = training(myModel, train_dl, num_epochs)\n",
    "#resume(myModel, 'best_model_CNN_Self_Dot.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume(myModel, 'best_model_CNN_Self_Dot.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.4973, -0.0000, -0.0000,  ..., -0.0000, -1.4812, -0.1336]],\n",
       " \n",
       "         [[ 0.0883,  0.3146,  0.0000,  ..., -0.0000,  0.0000, -0.8021]],\n",
       " \n",
       "         [[-0.0000,  1.0399, -0.0000,  ...,  0.5213,  0.0000,  2.9088]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.0745, -0.5943, -0.6714,  ..., -1.9792,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.3952,  1.0095,  1.1109,  ..., -0.0000,  0.0000, -0.0368]],\n",
       " \n",
       "         [[ 0.0000,  0.2458, -1.1569,  ..., -0.0000, -0.6918, -0.0974]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 1.5271, -0.2737,  0.1001,  ..., -1.6204,  0.0000, -0.7312]],\n",
       " \n",
       "         [[ 0.4554, -1.3962, -0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.5302,  0.0000,  1.1057,  ..., -0.0000,  0.0000, -0.8090]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.5764, -0.0000,  ..., -0.7033, -0.0000,  0.3688]],\n",
       " \n",
       "         [[-0.4761,  0.0000, -0.0000,  ...,  0.3449, -0.1074,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ..., -0.1202, -0.0000, -0.1229]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.6663, -1.1021, -0.0000,  ...,  0.0000, -1.6349, -0.0000]],\n",
       " \n",
       "         [[-0.6652, -0.4694, -0.9199,  ...,  0.0000, -0.1579,  0.8561]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.5589,  ...,  0.0691, -1.0222, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.5003, -0.0000, -0.0000,  ..., -0.0000,  0.7061, -0.0000]],\n",
       " \n",
       "         [[-0.9554,  0.0359, -0.6962,  ...,  0.0000, -0.0000,  0.4744]],\n",
       " \n",
       "         [[-0.6882, -0.0000, -0.0000,  ...,  0.6793, -0.0941,  0.3520]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000,  0.6099,  0.0000,  ...,  0.0000,  0.0803,  1.0206]],\n",
       " \n",
       "         [[ 0.0000,  0.6155,  0.0000,  ..., -1.4238,  0.7919, -0.8626]],\n",
       " \n",
       "         [[-0.5518, -0.0000, -0.0000,  ...,  0.3690,  0.2292,  0.3426]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0147, -0.0000, -1.0350,  ...,  0.0000, -0.0000,  0.5721]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  1.2429, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.8254, -0.0000,  ...,  0.0000, -0.3164,  0.8890]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.1866,  1.2113,  ..., -0.0000,  0.3270, -1.1467]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.4371,  ...,  1.1756, -1.0801,  1.4558]],\n",
       " \n",
       "         [[ 0.0000, -1.2524, -0.0000,  ..., -0.9883,  0.1612, -0.5069]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.9610, -0.0000, -1.1955,  ...,  1.4537, -0.0000,  1.1436]],\n",
       " \n",
       "         [[ 0.0000, -0.7471, -0.9544,  ..., -0.0000, -0.8970, -0.0000]],\n",
       " \n",
       "         [[-0.4105,  0.0000, -0.0000,  ..., -0.0827,  0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.0000,  0.4618,  ..., -1.0141,  1.2969,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.1438,  ...,  0.0809, -0.7281, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.6448,  ..., -0.0000, -0.6241, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4685,  0.0724,  0.0000,  ...,  0.0000, -0.0677,  0.5194]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.0000, -1.1315,  0.0000]],\n",
       " \n",
       "         [[-0.2264, -0.0000, -0.8726,  ...,  0.0000, -0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.9500,  1.2823,  0.0000,  ..., -0.0000,  0.8727, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.2588, -0.0000,  0.8857]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.4313, -0.5641,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.0000, -1.5421,  ...,  0.0000, -1.0206,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0791, -0.1739,  ..., -0.0000,  0.0000, -0.2332]],\n",
       " \n",
       "         [[ 1.2898, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.9692]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.8158]],\n",
       " \n",
       "         [[ 0.0000, -0.4135,  0.0000,  ..., -0.5798,  0.0000, -0.9063]],\n",
       " \n",
       "         [[-0.0000,  0.0000,  0.0000,  ...,  1.0132, -0.0000, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.7336, -0.0633, -0.7900,  ...,  0.0000, -0.4925,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.4139,  ..., -0.0000, -0.0000, -1.9835]],\n",
       " \n",
       "         [[ 0.3599, -1.1972, -0.5981,  ..., -0.0000,  0.6173, -0.7876]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0454, -0.8925, -0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000, -0.0000,  ..., -1.4761, -0.9999, -0.0280]],\n",
       " \n",
       "         [[-1.0066, -0.0000, -0.8218,  ...,  0.0000, -0.6658,  0.7909]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.7158, -1.1665, -1.4247,  ...,  0.6400, -0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.5432,  1.2252,  0.8900,  ..., -0.6660,  0.3845, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.4499, -0.1821,  ..., -1.1324,  0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.5533,  0.0000, -0.6954,  ...,  0.6617,  0.0817,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.7588, -1.0817,  ...,  0.2818, -1.0836, -0.6136]],\n",
       " \n",
       "         [[ 0.3947,  0.0000,  0.0000,  ..., -0.6612,  0.5163, -1.0003]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.2353,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.8211,  0.3560,  ...,  0.6642,  0.2267,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.7734,  0.0000, -0.2793,  ...,  0.5386, -0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -1.7768,  ..., -1.6159,  0.0000,  0.2867]],\n",
       " \n",
       "         [[ 0.2929, -0.0000, -0.0000,  ..., -0.1154, -0.0000, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.9300,  0.4233,  0.3825,  ..., -0.0000,  0.9145, -0.7191]],\n",
       " \n",
       "         [[ 1.7339,  0.3501,  0.0000,  ..., -2.4427,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.2031,  ..., -0.2912,  0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000, -0.5360, -0.6057,  ...,  1.3224,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.6953, -0.7468, -0.0000,  ..., -1.0611,  0.5590, -0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.3288, -0.5072,  ...,  0.0350, -1.0425,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.5897,  0.5358,  ..., -0.3014,  0.0000, -0.5341]],\n",
       " \n",
       "         [[-0.9872, -0.0000, -0.0000,  ...,  0.9181, -1.0239,  0.3786]],\n",
       " \n",
       "         [[ 0.4271, -0.0000, -0.6151,  ...,  0.0000, -1.2658,  0.3570]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.2768,  0.0000,  0.0000,  ..., -0.0000,  1.2509,  0.0000]],\n",
       " \n",
       "         [[-0.0597,  0.8312, -0.7573,  ..., -0.0000, -0.2713,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.8838,  ...,  0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.8071, -0.0000, -0.1919,  ..., -0.9642,  0.0000,  0.0083]],\n",
       " \n",
       "         [[ 0.0000,  0.0156,  0.0000,  ..., -0.5711,  0.7866, -0.8533]],\n",
       " \n",
       "         [[ 0.0000,  0.1635, -0.3966,  ..., -0.0000, -0.0000, -0.5994]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.0000, -0.0000,  ..., -0.4988,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.6673,  0.0000,  ...,  1.4020, -0.7889,  0.3278]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.2594, -1.0307, -0.0000,  ..., -2.8534,  0.5330, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.9162]],\n",
       " \n",
       "         [[-0.0552, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000,  0.9248,  0.0000,  ...,  0.2241, -0.1934,  0.0000]],\n",
       " \n",
       "         [[-1.2544, -1.1270, -0.0000,  ...,  1.3039, -0.0274,  0.0000]],\n",
       " \n",
       "         [[ 0.8366, -0.0000, -0.4746,  ..., -1.1470,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.3867,  1.9774]],\n",
       " \n",
       "         [[-0.0179, -0.0000, -1.1620,  ...,  0.0068, -0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.2908, -0.0000, -0.0000,  ...,  0.0000, -0.2503, -0.4250]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -1.0523, -0.5140,  ..., -0.7765,  0.6951, -0.0000]],\n",
       " \n",
       "         [[ 1.0452, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.3377]],\n",
       " \n",
       "         [[ 0.2805,  0.0000,  0.0000,  ..., -0.0000,  0.1770, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.0000, -1.4325,  ..., -1.8624, -0.7297, -0.0000]],\n",
       " \n",
       "         [[-0.9148, -0.5612, -0.0000,  ...,  0.0000,  0.2554,  1.4579]],\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.3242,  ...,  0.4628, -0.6345,  1.6738]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.0000,  1.3844,  ..., -2.5403,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 1.3762, -0.7027, -0.0000,  ..., -0.0000, -0.0000, -0.0000]],\n",
       " \n",
       "         [[-1.1754, -0.0000, -0.0000,  ...,  1.2917,  0.3161,  0.3742]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000, -0.1474,  ..., -0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.7444, -1.6132,  0.4376]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.8050, -0.0000,  ..., -0.4108, -0.7599, -0.0000]],\n",
       " \n",
       "         [[ 0.5456,  0.2685, -0.1471,  ..., -0.6084,  0.0000,  0.6365]],\n",
       " \n",
       "         [[-0.0000,  0.0000,  0.0275,  ...,  0.1622,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.2564, -0.7346, -0.6962,  ..., -1.7658,  0.5990, -0.0000]],\n",
       " \n",
       "         [[-0.4851, -0.3851, -0.6833,  ...,  0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.6733, -0.0000, -0.0000,  ...,  0.0000,  1.2532,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000e+00, -5.4532e-02, -1.0226e-03,  ...,  3.2622e-01,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         [[-0.0000e+00,  3.9299e-02,  2.5378e-02,  ...,  5.2649e-01,\n",
       "           -0.0000e+00, -7.9260e-02]],\n",
       " \n",
       "         [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  4.2658e-01,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.4923e+00, -3.3973e-01,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            9.3846e-01, -0.0000e+00]],\n",
       " \n",
       "         [[ 1.0113e+00, -0.0000e+00,  1.4370e-01,  ..., -0.0000e+00,\n",
       "            2.2691e-01, -0.0000e+00]],\n",
       " \n",
       "         [[ 3.5799e-01,  0.0000e+00,  0.0000e+00,  ..., -1.4066e-01,\n",
       "            0.0000e+00, -1.1647e-01]]], device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.1229,  0.0000, -0.0000,  ..., -0.0000,  0.8689,  0.0000]],\n",
       " \n",
       "         [[ 0.1850, -1.4856, -0.0000,  ...,  0.0000, -0.0000, -0.5041]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.3195]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000,  0.2061,  0.0000,  ...,  0.0000,  0.1681, -0.4216]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  1.0331,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-1.2693, -0.3401, -0.0000,  ...,  1.2399, -0.0000,  1.3555]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 1.0693,  0.5607, -0.0000,  ..., -1.5571,  0.7226, -0.3097]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.9375, -0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.4052,  1.0708,  ..., -0.0000,  0.0999, -1.5973]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.4913,  0.0000, -0.1917,  ..., -0.5070,  0.9932,  0.8312]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -1.1328, -0.4597]],\n",
       " \n",
       "         [[-1.0124, -0.5155, -0.0000,  ...,  0.4236,  0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -1.3770, -0.2033,  ..., -0.0000,  1.5782, -0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.0298,  ...,  0.0000, -0.7888,  1.0883]],\n",
       " \n",
       "         [[ 0.0000, -0.2498, -1.3593,  ..., -0.0000, -0.0000, -0.9029]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.9678, -0.0000, -1.1691,  ..., -1.5512,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.2277,  0.0000,  0.7496,  ...,  0.1137,  0.3985,  0.0000]],\n",
       " \n",
       "         [[-0.0521,  0.8174,  0.0000,  ..., -0.0000, -0.3792, -0.1737]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.0000, -0.6761,  ..., -1.7016, -0.0000, -1.5041]],\n",
       " \n",
       "         [[ 0.9971,  0.0000, -0.0000,  ..., -0.7755, -0.2471, -0.5629]],\n",
       " \n",
       "         [[-0.0000, -0.6058, -0.6187,  ...,  1.1592, -0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.6067,  0.1207, -0.0281,  ..., -0.0000, -0.9372, -0.7369]],\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.0000,  ...,  0.1983, -0.0000,  0.8695]],\n",
       " \n",
       "         [[-0.2519, -0.0000, -0.0000,  ...,  0.3598,  0.1045,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000, -0.2227, -0.0000,  ...,  0.3632, -0.0000, -0.8021]],\n",
       " \n",
       "         [[ 0.6349,  0.0000,  0.0000,  ..., -1.0043,  0.9236, -0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.1822,  0.6110,  ..., -0.3020,  0.0000, -0.7695]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.4049, -0.0000,  ...,  0.0000, -0.0000,  1.7438]],\n",
       " \n",
       "         [[ 0.0000,  0.0000, -1.0830,  ..., -1.3941, -0.0000,  0.0858]],\n",
       " \n",
       "         [[ 0.6625,  0.0000, -0.3473,  ..., -0.0000,  0.0000,  0.5216]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.2396,  0.0000,  0.9514,  ...,  0.0990, -0.0843,  0.3199]],\n",
       " \n",
       "         [[-0.2315, -0.0000, -1.9072,  ..., -0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.6546,  0.5075,  ..., -0.0000,  0.0000,  0.5241]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.6254,  ...,  0.2908, -0.0000, -0.4170]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -1.8359, -1.0506]],\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.4849,  ...,  0.5513, -0.3401,  1.0060]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.6240, -1.0099, -0.8948,  ..., -0.3800, -0.0000,  0.0204]],\n",
       " \n",
       "         [[-1.2190, -0.0000, -1.5251,  ...,  0.0000, -1.0379, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -1.0928, -0.0000,  ..., -0.0000, -0.0776,  0.3140]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.4363, -0.0000]],\n",
       " \n",
       "         [[ 0.0000,  1.1602,  0.3431,  ..., -0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.0000,  0.0000,  ..., -0.0473, -0.1494,  0.2784]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 1.0518,  0.0000,  0.0000,  ..., -0.0000,  0.3785, -0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0151, -0.1545,  ..., -0.0000, -0.0759,  0.0028]],\n",
       " \n",
       "         [[-0.0000, -0.2574, -1.2706,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ..., -0.0753,  0.7110,  0.2095]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.4223,  ..., -0.0000, -0.0000, -0.1008]],\n",
       " \n",
       "         [[-0.4164,  0.0000,  0.0000,  ...,  0.0000, -0.1928,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.2448, -0.0000,  ..., -0.2904, -0.4516, -0.0000]],\n",
       " \n",
       "         [[ 0.2582,  0.0000,  0.0000,  ..., -0.0000,  1.3370,  1.6909]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0664,  ...,  0.5698,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.6119, -0.0000, -0.0000,  ...,  0.0000, -0.5111, -0.0000]],\n",
       " \n",
       "         [[ 0.4464, -0.0000, -1.2906,  ..., -0.0000, -0.0000, -0.0000]],\n",
       " \n",
       "         [[ 1.1727,  0.4160, -0.0000,  ..., -1.8428,  0.0939, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.2887, -1.5769, -0.0000,  ..., -0.0000,  0.0000, -0.3921]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.8844,  ..., -0.9779,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.5670,  0.0082, -0.0000,  ..., -0.0389, -0.3278,  0.1380]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.5377, -1.1277,  ..., -0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.6154,  0.3739,  0.0000,  ..., -1.1548,  0.0000, -0.7811]],\n",
       " \n",
       "         [[-0.6815, -0.9050, -0.0000,  ...,  0.0000, -0.7500, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.5059]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  1.3196,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -1.0309, -1.5500,  ..., -0.1406, -0.0000, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000,  1.1049,  ..., -0.8204,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.4483, -0.3742, -0.2355,  ...,  0.0860, -0.0000, -0.3218]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -1.8605,  ...,  1.4399, -0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.7847, -1.8506, -2.3589,  ...,  0.6331, -0.2310,  0.0000]],\n",
       " \n",
       "         [[ 2.2439, -0.6540, -0.7483,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.3247,  0.8084,  0.8757,  ..., -0.4512,  0.6830, -0.3408]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.0000,  0.0000,  ...,  0.3407, -0.2444, -0.2540]],\n",
       " \n",
       "         [[ 0.6871, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.0634, -0.0000,  ...,  0.0000, -0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -1.0329, -1.4683,  ...,  0.0000, -0.4989,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -1.1123,  ...,  0.0000, -0.9763,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.4900, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.3844]],\n",
       " \n",
       "         [[ 0.7365, -0.0071, -0.7895,  ..., -0.0000, -0.6840, -0.0000]],\n",
       " \n",
       "         [[ 0.5342,  0.0000,  0.1276,  ..., -0.5321, -0.0000,  0.3998]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.5858, -0.2440, -0.0000,  ..., -0.8800, -0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.0908, -0.0000, -0.0000,  ..., -1.0083,  1.0717,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.1062, -0.0000,  ..., -0.0531,  0.6590,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 3.0974, -1.3875, -0.8133,  ..., -3.0653,  1.0341, -2.3148]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.6258,  ..., -0.0000,  0.0000, -1.0692]],\n",
       " \n",
       "         [[-0.0000,  0.0000,  0.7767,  ...,  0.0472,  0.0000,  0.6406]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.6336, -0.0000, -0.0000,  ...,  0.3070,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.5596,  ..., -1.1439,  0.7243, -0.0000]],\n",
       " \n",
       "         [[ 0.3988, -0.0000, -0.0000,  ..., -0.4818,  0.0000, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0494, -0.1471,  0.0000,  ..., -0.1152,  0.7246, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000,  0.1379,  ..., -0.0000, -1.4180, -0.0000]],\n",
       " \n",
       "         [[ 0.7839, -0.0000, -1.4798,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.4698,  0.0000,  0.0000,  ..., -0.5862,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.1727, -0.1213,  ...,  0.1674,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.2804, -0.0000,  ...,  0.8157, -0.0000,  1.5436]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -1.5394, -1.3201,  ..., -0.8880,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.1463, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.6744]],\n",
       " \n",
       "         [[ 1.3280, -0.0000, -0.8424,  ..., -0.0000, -0.0000, -1.8778]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.7994,  1.7012]],\n",
       " \n",
       "         [[ 0.5115,  1.5528,  0.8808,  ..., -0.6876,  0.3060, -0.0000]],\n",
       " \n",
       "         [[-0.4816, -0.8068, -1.6062,  ...,  0.6222, -1.6097, -0.7900]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  1.5858,  1.6658,  ..., -0.0000,  0.0000, -0.6454]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  1.0769]],\n",
       " \n",
       "         [[ 0.6970,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0336]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.2541, -0.0000, -0.0000,  ...,  0.3799, -0.0000, -0.2080]],\n",
       " \n",
       "         [[-0.0118,  0.0000, -1.3267,  ..., -0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.6297, -1.1937, -1.2129,  ...,  0.0000, -0.3952, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.6274,  0.1563, -0.4415]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.9676,  ...,  1.0223, -0.6070,  0.9023]],\n",
       " \n",
       "         [[ 0.3763, -0.0000, -0.0000,  ..., -0.0000, -0.7407, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.0000, -0.8765,  ..., -0.8585, -1.0801, -0.8965]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.6324,  ..., -0.0000,  0.0000, -0.2460]],\n",
       " \n",
       "         [[-0.0000, -0.7344, -0.0000,  ...,  0.7850, -0.8600,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.2395,  0.7869,  0.0000,  ..., -0.0000,  0.6918,  0.0000]],\n",
       " \n",
       "         [[ 0.6343,  0.2209, -0.0000,  ..., -0.5566,  0.0000,  0.5797]],\n",
       " \n",
       "         [[ 0.7795,  0.2253, -0.3245,  ..., -1.0844,  0.0000, -0.2533]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -1.2670, -0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.2040,  0.0000,  0.0246,  ..., -0.0000, -0.0067, -0.5041]],\n",
       " \n",
       "         [[-0.5160,  0.0000,  0.0000,  ...,  0.5841, -0.1398,  0.4259]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  1.7004,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.4752, -0.0000,  ...,  0.0000,  0.0000,  0.6129]],\n",
       " \n",
       "         [[ 1.4123, -1.2203, -0.0000,  ..., -1.8084,  0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.8244, -0.0000, -1.6011,  ..., -1.4098,  0.0000, -0.4700]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.3916, -0.0000, -0.4192]],\n",
       " \n",
       "         [[-2.3945,  0.1342, -1.0301,  ...,  1.9979, -0.0000,  1.9212]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.4096, -0.0000, -1.3506,  ..., -0.0000, -0.5499, -0.0000]],\n",
       " \n",
       "         [[ 0.8217,  0.0226,  0.0796,  ..., -0.0000, -0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.2615, -0.1577,  ...,  0.0000,  1.3256,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.0000, -0.0000,  ...,  0.0170, -0.8406,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000, -0.0000,  ..., -0.3539, -0.2117, -0.0000]],\n",
       " \n",
       "         [[-0.9424, -0.0000, -1.2115,  ...,  0.0000, -0.2712,  0.7256]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000,  0.3696,  0.5567,  ...,  0.1471,  0.0000,  0.4406]],\n",
       " \n",
       "         [[ 0.3401,  0.0000, -0.0765,  ..., -0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.5661,  0.7156, -0.4324,  ...,  0.0328,  0.4471,  1.4154]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.2152,  0.0000,  0.0000,  ...,  0.3146,  0.3172,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.6952, -0.0000,  ...,  1.4495, -0.0703,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.6973,  ...,  1.0757,  0.3621,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000, -0.0000,  ..., -0.3566,  0.9871,  0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.5437, -0.0000,  ..., -0.4127,  1.1339,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -1.7852, -1.6360,  ..., -1.1502,  0.4333,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.3731,  0.0000,  ..., -0.0000,  0.6184,  0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.0764, -1.0430,  ...,  1.3574, -0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.0081,  0.7819,  0.0000,  ...,  0.6643, -0.0000,  1.3317]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.2240, -0.0000, -0.3385,  ...,  0.2258,  0.0000,  0.1435]],\n",
       " \n",
       "         [[ 0.2193, -0.5903, -0.0000,  ..., -0.2208,  0.5648, -0.7586]],\n",
       " \n",
       "         [[ 1.0429, -0.1113, -0.4730,  ..., -0.8514,  0.0927, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.7675, -0.8377, -1.0734,  ...,  0.0000,  0.3011,  0.5717]],\n",
       " \n",
       "         [[-0.4776,  0.5707, -0.0000,  ...,  0.3632, -0.0000,  0.5100]],\n",
       " \n",
       "         [[-0.0000,  0.0000,  0.7847,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.2876,  ...,  0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.3045, -0.5886, -0.0000,  ..., -0.2673, -1.4305, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -1.3522, -0.0000,  ..., -0.6937, -0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.7593,  0.6759, -0.0000,  ..., -0.0000,  0.0000, -0.3130]],\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.5442,  ...,  0.8491,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ...,  0.3188, -1.0237,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.3674, -0.0000,  ..., -0.0000,  0.8416, -0.0000]],\n",
       " \n",
       "         [[ 0.1639,  0.1580,  0.0000,  ..., -0.7769,  0.8510, -0.0000]],\n",
       " \n",
       "         [[-0.0297,  0.0000,  0.6438,  ..., -0.1187, -0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000,  0.0000, -1.2990,  ...,  0.9124, -0.3966,  1.3281]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  1.0595,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.1770,  0.0000,  ...,  0.1556, -0.0000, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0913,  0.4664,  0.0000,  ...,  0.1338,  0.5312, -0.0443]],\n",
       " \n",
       "         [[-0.1571,  0.0000,  0.0000,  ...,  0.1419,  0.9410,  0.9730]],\n",
       " \n",
       "         [[ 1.2874, -1.1636, -0.0000,  ..., -1.7918,  0.5974, -0.5139]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 1.0065, -0.5554, -0.0000,  ..., -0.0000, -0.6728, -0.0000]],\n",
       " \n",
       "         [[ 0.6750,  0.4603, -0.0000,  ..., -0.0000, -0.6673, -0.0084]],\n",
       " \n",
       "         [[-0.0584, -0.0000, -0.8636,  ...,  0.0265,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.3423, -0.0000, -0.8055,  ...,  0.4424,  0.0000,  1.2576]],\n",
       " \n",
       "         [[-0.3012, -0.0000, -0.0000,  ...,  0.2566, -0.1363,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.4188,  0.0000,  ..., -0.0000,  0.8495, -1.6881]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 1.4552,  0.5380, -0.0000,  ..., -1.4784, -0.1587, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.1532,  ..., -0.6182,  0.3814, -0.6453]],\n",
       " \n",
       "         [[ 0.0000, -0.5633, -0.6583,  ..., -0.0000, -0.2722, -0.2764]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0361, -0.0000, -0.0000,  ...,  0.0000, -0.3323,  0.5870]],\n",
       " \n",
       "         [[-1.7647, -1.5689, -2.1165,  ...,  1.5233, -0.0000,  1.9798]],\n",
       " \n",
       "         [[-1.3916, -0.4568, -0.0000,  ...,  0.8496,  0.8801,  0.8793]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.0000, -0.8513,  ...,  0.0000, -0.8613,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.6869,  ...,  0.0000,  0.3773,  0.0000]],\n",
       " \n",
       "         [[ 1.7540,  1.3865, -0.0000,  ..., -2.2521,  1.7361,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ..., -0.0275,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.2366, -1.0728, -0.7189,  ..., -0.0000, -0.6932, -0.0000]],\n",
       " \n",
       "         [[ 1.8809, -0.0000, -1.3360,  ..., -0.0000,  0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0935,  0.0000, -1.0540,  ..., -0.6145, -0.0000, -0.1317]],\n",
       " \n",
       "         [[-1.2282, -0.0000, -0.9156,  ...,  0.0000, -0.0000,  0.7164]],\n",
       " \n",
       "         [[ 0.3302, -0.0000, -1.4384,  ..., -0.5351,  0.7237,  0.3242]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.6821,  0.3546,  1.0259,  ..., -1.2097,  0.7951, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000,  0.1921,  ..., -0.0000, -0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.4015,  0.0879, -0.3272,  ...,  0.6556, -0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0280,  0.0000,  0.3710,  ..., -0.0907,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -1.2523, -0.0000,  ...,  0.0000,  0.0000,  0.8887]],\n",
       " \n",
       "         [[-1.3468, -0.6965, -0.0000,  ...,  0.0000,  0.5652,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.6646, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -1.6760,  ...,  0.0000, -0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.9808, -0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000, -0.6704, -0.0000,  ...,  0.5301, -0.2853,  0.4276]],\n",
       " \n",
       "         [[-0.0000, -0.0963, -0.0000,  ...,  0.9837,  0.5630,  0.4241]],\n",
       " \n",
       "         [[-0.5551, -0.6171, -0.4913,  ...,  0.0763,  0.0000,  0.1409]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.9441, -0.4111, -0.5912,  ...,  0.5549,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.4698,  0.0000,  ..., -0.9058,  1.0779,  0.1443]],\n",
       " \n",
       "         [[-0.0000,  0.4257, -0.0000,  ...,  0.0000, -0.0000,  1.4228]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-1.7613, -0.0000, -1.3914,  ...,  1.7431, -0.1539,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0831, -0.5475,  ...,  1.0536,  0.0000,  1.2489]],\n",
       " \n",
       "         [[ 0.1045,  1.0561,  0.0148,  ..., -0.2366,  0.0000,  1.1592]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.6584, -1.4922, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.7779, -0.0000,  ..., -0.0000,  1.3033, -0.0000]],\n",
       " \n",
       "         [[ 1.4762, -0.8957, -0.8259,  ..., -0.0000, -0.0000, -1.0733]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.2782, -0.0000, -0.7872,  ..., -0.0000, -0.0000, -1.0861]],\n",
       " \n",
       "         [[ 0.0000, -0.6691, -0.0000,  ..., -0.2343, -0.1814, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -1.2080,  ...,  1.5757, -0.0000,  1.8455]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.3959,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.0000, -1.0388, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.8080, -0.7455, -0.7301,  ..., -0.0000,  0.8218, -0.4476]],\n",
       " \n",
       "         [[ 0.0000, -1.2521, -1.4964,  ..., -0.0000,  1.2048,  0.1976]],\n",
       " \n",
       "         [[ 1.5585, -0.2807, -0.0000,  ..., -1.5780,  0.7991, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.9704, -0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ...,  0.2461, -0.0000, -1.1227]],\n",
       " \n",
       "         [[ 0.0000, -0.4289, -0.0000,  ..., -0.6375, -0.8028, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000e+00,  1.0866e+00, -2.0124e-02,  ..., -3.7495e-01,\n",
       "            1.4180e+00,  1.4966e+00]],\n",
       " \n",
       "         [[ 4.6152e-01,  9.8580e-01,  0.0000e+00,  ..., -8.6631e-01,\n",
       "            0.0000e+00, -5.2523e-01]],\n",
       " \n",
       "         [[-9.4248e-01, -2.4524e-01, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           -5.7134e-01,  0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000e+00, -7.0405e-01, -0.0000e+00,  ..., -2.0994e-01,\n",
       "           -0.0000e+00, -8.8940e-01]],\n",
       " \n",
       "         [[-7.0824e-01, -0.0000e+00, -9.2270e-01,  ...,  4.6908e-01,\n",
       "            3.7428e-05,  0.0000e+00]],\n",
       " \n",
       "         [[-1.5547e+00, -1.3602e+00, -0.0000e+00,  ...,  5.8117e-01,\n",
       "            1.2609e+00,  9.7774e-01]]], device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-1.1607e+00, -4.2837e-01, -0.0000e+00,  ...,  1.3065e+00,\n",
       "           -0.0000e+00,  5.7662e-01]],\n",
       " \n",
       "         [[ 0.0000e+00,  8.3496e-01,  0.0000e+00,  ..., -1.2030e+00,\n",
       "            1.2623e+00, -0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00, -1.1028e-01, -2.4937e-01,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 4.6090e-01, -0.0000e+00, -4.7118e-01,  ..., -2.3861e-01,\n",
       "           -1.2904e+00, -0.0000e+00]],\n",
       " \n",
       "         [[-0.0000e+00, -3.4944e-01, -9.6515e-01,  ...,  1.5837e+00,\n",
       "           -2.5821e-01,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  1.5622e-03,  ..., -7.5612e-01,\n",
       "            3.7633e-01,  3.2250e-01]]], device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 1.0957, -0.0000, -0.5369,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.6708,  1.3356,  0.7632,  ..., -0.0000,  1.3820,  0.0000]],\n",
       " \n",
       "         [[-1.8637, -0.3582, -0.4768,  ...,  1.6213,  0.6331,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  ...,  0.3596, -1.5172, -0.9481]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.8171,  ..., -0.0000,  0.0000, -0.7379]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -1.2703,  ..., -0.0000,  0.1662, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.0000, -0.6131,  ...,  0.0000, -0.5744, -1.2194]],\n",
       " \n",
       "         [[ 0.0000, -0.7056, -0.4286,  ..., -0.0000, -0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.7741,  0.0000,  0.0000,  ...,  0.0000,  0.4919,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.6698, -0.3614, -0.6360,  ..., -0.0000,  0.7074, -0.7013]],\n",
       " \n",
       "         [[-0.8182, -0.7212, -0.0000,  ...,  1.4049, -0.0537,  0.0000]],\n",
       " \n",
       "         [[ 0.7087, -0.2647, -0.0000,  ..., -0.0000, -0.4946, -1.0840]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.5663,  0.5896, -0.0000,  ..., -0.0000,  0.0000,  0.4298]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -1.0198,  0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.6555, -0.2882, -0.0000,  ..., -0.8242,  0.0000, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.0791, -0.7255,  ..., -0.0000, -0.4402, -0.3042]],\n",
       " \n",
       "         [[ 0.8967, -1.3104, -2.2529,  ..., -0.0000, -0.0000, -1.8889]],\n",
       " \n",
       "         [[ 0.0000,  0.6453,  0.0000,  ..., -0.0000,  0.8313, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000,  0.3310,  0.0000,  ...,  0.6521, -0.0000,  0.0000]],\n",
       " \n",
       "         [[ 1.1914,  0.0000, -0.0000,  ..., -0.0000,  0.4007, -0.1380]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.1033, -1.0359, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.3181, -0.0000, -0.0000,  ...,  1.4437, -0.4029,  1.4372]],\n",
       " \n",
       "         [[ 1.2541,  0.0000,  0.3906,  ..., -0.0000,  1.2714,  0.0000]],\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.0728,  ..., -0.0332,  0.0000,  0.6293]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.4140]],\n",
       " \n",
       "         [[-0.0000,  0.3302, -0.0000,  ...,  0.0000,  0.1523,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -1.8308,  ..., -0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0000, -0.0000, -1.0032,  ...,  0.0000, -0.0000,  0.2719]],\n",
       " \n",
       "         [[-0.7771, -0.4481, -0.0000,  ...,  0.0000, -0.6164,  0.0194]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -1.4287,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.7411,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -1.1710,  ...,  1.1595, -0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -1.6132,  ...,  0.0000, -0.0000, -1.0299]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.9375,  0.0000, -1.1252,  ...,  0.9846, -0.3972,  1.0460]],\n",
       " \n",
       "         [[ 0.6011, -0.9887, -0.4556,  ..., -0.0000,  0.1061, -0.3157]],\n",
       " \n",
       "         [[ 0.0000, -1.3162, -0.0000,  ..., -0.0000,  1.3276, -0.3622]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.7953, -0.5954, -0.7975,  ..., -1.0565, -0.7541, -0.0000]],\n",
       " \n",
       "         [[-0.2446,  0.0000,  0.0000,  ...,  0.0733,  0.2436, -0.0000]],\n",
       " \n",
       "         [[ 0.0257,  0.0840, -0.3814,  ..., -0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.6643,  0.0000,  0.0000,  ..., -0.7981,  0.0000,  0.4667]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ..., -1.5348,  0.2331, -0.0000]],\n",
       " \n",
       "         [[ 0.0000,  1.1514,  0.5602,  ..., -0.0000,  0.7059,  0.6695]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.9617,  0.0000, -0.0000,  ..., -1.2663,  0.0000,  0.2282]],\n",
       " \n",
       "         [[ 0.4703, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.5369]],\n",
       " \n",
       "         [[-0.8517, -0.0000, -1.5128,  ...,  1.4593, -1.6211,  0.7211]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.3060,  0.0000,  0.0000,  ..., -0.5801,  0.0000, -0.1328]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -2.5194,  ...,  0.7334, -0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0000, -1.2272, -0.0000,  ...,  0.0000, -0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-1.5549, -0.0000, -0.0000,  ...,  1.1148,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.8929, -2.1469,  ..., -0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.8127, -0.5373, -1.1182,  ...,  0.0000, -0.4688,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.6441, -0.0000, -1.0510,  ..., -0.0000,  0.0000, -0.1169]],\n",
       " \n",
       "         [[-0.2430,  0.4121,  0.0000,  ...,  0.0000, -0.0000,  0.2309]],\n",
       " \n",
       "         [[ 0.7728, -0.0000, -0.8448,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.5132, -0.0000,  ..., -1.3294, -2.0205, -0.0000]],\n",
       " \n",
       "         [[-0.6117, -0.0000, -0.7291,  ...,  0.0000,  0.0000,  0.3594]],\n",
       " \n",
       "         [[ 0.0000,  0.7374,  0.3383,  ..., -0.7933,  1.0148, -0.2384]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0320,  0.5202, -1.6352,  ...,  0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0783,  0.0000,  0.1187,  ..., -0.3727, -0.0000, -1.4988]],\n",
       " \n",
       "         [[-0.0000, -0.9772, -0.1728,  ...,  0.0000,  0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-0.0000,  0.0000, -0.0000,  ..., -0.1640,  0.8741,  0.0000]],\n",
       " \n",
       "         [[-0.0000,  1.0514,  0.0000,  ...,  0.0000,  0.3598, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  ..., -0.3311,  0.0000, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1865,  0.6057,  0.0000,  ..., -0.2615,  0.0000,  0.6183]],\n",
       " \n",
       "         [[-0.0000, -1.2184, -2.2834,  ...,  0.5068, -0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.7897,  1.0169,  ..., -0.4621,  0.0000,  0.3146]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0921, -0.0000, -0.0000,  ..., -0.2755, -0.2269, -0.3593]],\n",
       " \n",
       "         [[-0.0168, -0.6237, -0.4567,  ..., -0.1440,  0.4869,  0.7900]],\n",
       " \n",
       "         [[ 0.5232,  0.0000,  0.1543,  ..., -0.3803,  0.0000,  0.4325]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1005, -0.7820, -0.0000,  ..., -0.0000, -0.0000, -0.7176]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  ..., -0.7714,  0.7174, -0.1400]],\n",
       " \n",
       "         [[ 0.0000, -0.0000,  0.0000,  ..., -1.5560,  0.0000, -1.4790]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[-1.3923, -0.0000, -0.6074,  ...,  0.0000, -0.3246,  0.0000]],\n",
       " \n",
       "         [[ 1.2506, -0.0000, -0.0000,  ..., -1.4823,  0.5858, -0.0000]],\n",
       " \n",
       "         [[ 0.3244, -0.0000, -1.3579,  ..., -0.0000,  1.0641, -0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.4784,  0.2015, -0.0000,  ...,  1.5841, -0.6886,  1.6359]],\n",
       " \n",
       "         [[-0.0000, -0.5584, -0.0000,  ...,  0.0203, -0.0000, -0.0716]],\n",
       " \n",
       "         [[ 0.0000, -1.2033, -1.1066,  ..., -1.6736,  0.4238, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000, -0.7403, -0.8882,  ..., -0.0000, -0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.1479, -0.5521,  ..., -2.4912,  0.3125, -1.3708]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.5863,  ...,  0.0633, -0.2342, -0.1014]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000, -0.4713, -0.0000,  ..., -0.2511,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.0458, -0.2083, -1.2362,  ...,  0.0000, -0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.5777,  0.5112,  0.0000,  ..., -0.0000,  0.0000, -0.0000]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[[ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.5026, -0.0000]],\n",
       " \n",
       "         [[ 0.3872,  0.7935,  0.0000,  ..., -0.0386,  0.0000,  0.0986]],\n",
       " \n",
       "         [[ 0.0901, -1.3269, -0.0000,  ..., -0.1522, -1.1685, -1.4541]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000,  0.2113,  0.1678,  ...,  0.0000, -0.3017,  0.6117]],\n",
       " \n",
       "         [[-0.0000,  0.0000, -0.6425,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.4752,  2.2639,  0.9235,  ..., -1.3472,  1.7084,  1.7634]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(atten_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.5026, -0.0000]],\n",
       "\n",
       "        [[ 0.3872,  0.7935,  0.0000,  ..., -0.0386,  0.0000,  0.0986]],\n",
       "\n",
       "        [[ 0.0901, -1.3269, -0.0000,  ..., -0.1522, -1.1685, -1.4541]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.2113,  0.1678,  ...,  0.0000, -0.3017,  0.6117]],\n",
       "\n",
       "        [[-0.0000,  0.0000, -0.6425,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4752,  2.2639,  0.9235,  ..., -1.3472,  1.7084,  1.7634]]],\n",
       "       device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_map[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(atten_map[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 0, 6, 6, 1, 1, 1, 4, 4, 2, 1, 5, 5, 0, 4, 2], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_names = []\n",
    "for label in labels_tens[-1]:\n",
    "    if label == 1:\n",
    "        emotion_names.append('neutral')\n",
    "    elif label == 2:\n",
    "        emotion_names.append('calm')\n",
    "    elif label == 3:\n",
    "        emotion_names .append('happy')\n",
    "    elif label == 4:\n",
    "        emotion_names.append('sad')\n",
    "    elif label == 5:\n",
    "        emotion_names.append('angry')\n",
    "    elif label == 6:\n",
    "        emotion_names.append('fearful')\n",
    "    else:\n",
    "        emotion_names.append('disgust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25412\\1208891806.py:21: UserWarning: Only one segment is calculated since parameter NFFT (=256) >= signal length (=64).\n",
      "  axes[row_index, col_index].specgram(data_array.flatten(), Fs=44100)  # Adjust Fs accordingly\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Assuming you have a list of 16 tensors\n",
    "# Replace this with your actual list of tensors\n",
    "tensor_list = atten_map[-1]\n",
    "\n",
    "# Create a 4x4 subplot grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "# Iterate through the tensors and plot them in subplots\n",
    "for i, tensor_data in enumerate(tensor_list):\n",
    "    # Convert the tensor to a numpy array\n",
    "    data_array = tensor_data.cpu().detach().numpy()\n",
    "\n",
    "    # Calculate the row and column indices for the subplot grid\n",
    "    row_index = i // 4\n",
    "    col_index = i % 4\n",
    "\n",
    "    # Plot the spectrogram in the current subplot\n",
    "    axes[row_index, col_index].specgram(data_array.flatten(), Fs=44100)  # Adjust Fs accordingly\n",
    "    axes[row_index, col_index].set_title(emotion_names[i])\n",
    "    axes[row_index, col_index].set_xlabel('Time')\n",
    "    axes[row_index, col_index].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  # Disable gradient updates\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      # Get the input features and target labels, and put them on the GPU\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "      # Normalize the inputs\n",
    "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "      inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "      # Get predictions\n",
    "      outputs, atten_map = model(inputs)\n",
    "\n",
    "      # Get the predicted class with the highest score\n",
    "      _, prediction = torch.max(outputs,1)\n",
    "      # Count of predictions that matched the target label\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "\n",
    "  acc = correct_prediction/total_prediction\n",
    "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodel = AudioClassifier()\n",
    "bestmodel.load_state_dict(torch.load(r'C:\\Users\\User\\9444_WeightDancers_proj\\best_model_CNN_Self_Dot.pth'))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "bestmodel = bestmodel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(bestmodel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74, Total items: 288\n"
     ]
    }
   ],
   "source": [
    "# Run inference on trained model with the validation set\n",
    "inference(bestmodel, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7ifrHZBCFvK"
   },
   "source": [
    "### Taking self attention in Convolutional Neural Networks\n",
    "\n",
    "https://medium.com/mlearning-ai/self-attention-in-convolutional-neural-networks-172d947afc00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai\n",
      "  Downloading fastai-2.7.13-py3-none-any.whl (232 kB)\n",
      "Requirement already satisfied: pip in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (21.2.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (1.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (2.27.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (1.0.2)\n",
      "Collecting fastcore<1.6,>=1.5.29\n",
      "  Downloading fastcore-1.5.29-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: torchvision>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (0.13.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (9.0.1)\n",
      "Requirement already satisfied: torch<2.2,>=1.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (1.12.0)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (1.7.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (21.3)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Collecting spacy<4\n",
      "  Downloading spacy-3.7.2-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastai) (3.5.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (2.11.3)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (61.2.0)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.1-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (4.64.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->fastai) (3.0.4)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting pydantic-core==2.10.1\n",
      "  Downloading pydantic_core-2.10.1-cp39-none-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->fastai) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->fastai) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->fastai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->fastai) (1.26.9)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4->fastai) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai) (8.0.4)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->spacy<4->fastai) (2.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->fastai) (2020.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->fastai) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->fastai) (2.2.0)\n",
      "Installing collected packages: typing-extensions, pydantic-core, colorama, catalogue, annotated-types, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, fastprogress, fastcore, spacy, fastdownload, fastai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.5.0\n",
      "    Uninstalling typing-extensions-4.5.0:\n",
      "      Successfully uninstalled typing-extensions-4.5.0\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 colorama-0.4.6 confection-0.1.3 cymem-2.0.8 fastai-2.7.13 fastcore-1.5.29 fastdownload-0.0.7 fastprogress-1.0.3 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.4.2 pydantic-core-2.10.1 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 typing-extensions-4.8.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699661533690,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "OfwMfKHfCFXN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from torch.nn import Module, Parameter\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from fastai.layers import ConvLayer, NormType\n",
    "\n",
    "\n",
    "class SelfAttention(Module):\n",
    "    \"Self attention layer for `n_channels`.\"\n",
    "    def __init__(self, n_channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query,self.key,self.value = [self._conv(n_channels, c) for c in (n_channels//8,n_channels//8,n_channels)]\n",
    "        self.gamma = nn.Parameter(tensor([0.]))\n",
    "\n",
    "    def _conv(self,n_in,n_out):\n",
    "        return ConvLayer(n_in, n_out, ks=1, ndim=1, norm_type=NormType.Spectral, act_cls=None, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Notation from the paper.\n",
    "        size = x.size()\n",
    "        x = x.view(*size[:2],-1)\n",
    "        f,g,h = self.query(x),self.key(x),self.value(x)\n",
    "        beta = F.softmax(torch.bmm(f.transpose(1,2), g), dim=1)\n",
    "        o = self.gamma * torch.bmm(h, beta) + x\n",
    "        return o.view(*size).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699661535683,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "xAjOIfuiK12s",
    "outputId": "ed1b94b6-2d3c-4d9b-8516-19b4b7b51874"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        ##weight initialization by kaiming's method\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        \n",
    "        ##First Convolution Attention layer\n",
    "        self.selfattention1 = SelfAttention(n_channels = 8)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1, self.selfattention1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        \n",
    "        ##Second Convolution Attention layer\n",
    "        self.selfattention2 = SelfAttention(n_channels = 16)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2, self.selfattention2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        \n",
    "        ##Third Convolution Attention Layer\n",
    "        self.selfattention3 = SelfAttention(n_channels = 32)\n",
    "        self.conv3.bias.data.zero_()\n",
    "\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3, self.selfattention3]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        \n",
    "        ##Fourth Convolution Attention Layer\n",
    "        self.selfattention4 = SelfAttention(n_channels = 64)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4, self.selfattention4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=8)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqZVMNabYb8a"
   },
   "source": [
    "#### Training of the CNN + Convolutional Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1699661541259,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "C5YyH-5Ttf81"
   },
   "outputs": [],
   "source": [
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4860063,
     "status": "ok",
     "timestamp": 1699666471412,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "ui0Kz0_rYZNu",
    "outputId": "c8668978-8a16-4a7a-d8e8-250efe830a2d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25412\\3700501745.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tens = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.06, Accuracy: 0.14\n",
      "Epoch: 1, Loss: 1.97, Accuracy: 0.25\n",
      "Epoch: 2, Loss: 1.91, Accuracy: 0.26\n",
      "Epoch: 3, Loss: 1.86, Accuracy: 0.28\n",
      "Epoch: 4, Loss: 1.84, Accuracy: 0.28\n",
      "Epoch: 5, Loss: 1.81, Accuracy: 0.29\n",
      "Epoch: 6, Loss: 1.78, Accuracy: 0.30\n",
      "Epoch: 7, Loss: 1.77, Accuracy: 0.33\n",
      "Epoch: 8, Loss: 1.74, Accuracy: 0.32\n",
      "Epoch: 9, Loss: 1.72, Accuracy: 0.34\n",
      "Epoch: 10, Loss: 1.69, Accuracy: 0.34\n",
      "Epoch: 11, Loss: 1.66, Accuracy: 0.36\n",
      "Epoch: 12, Loss: 1.64, Accuracy: 0.37\n",
      "Epoch: 13, Loss: 1.61, Accuracy: 0.39\n",
      "Epoch: 14, Loss: 1.58, Accuracy: 0.41\n",
      "Epoch: 15, Loss: 1.57, Accuracy: 0.40\n",
      "Epoch: 16, Loss: 1.51, Accuracy: 0.43\n",
      "Epoch: 17, Loss: 1.46, Accuracy: 0.45\n",
      "Epoch: 18, Loss: 1.45, Accuracy: 0.45\n",
      "Epoch: 19, Loss: 1.43, Accuracy: 0.46\n",
      "Epoch: 20, Loss: 1.38, Accuracy: 0.48\n",
      "Epoch: 21, Loss: 1.36, Accuracy: 0.50\n",
      "Epoch: 22, Loss: 1.31, Accuracy: 0.52\n",
      "Epoch: 23, Loss: 1.29, Accuracy: 0.52\n",
      "Epoch: 24, Loss: 1.25, Accuracy: 0.56\n",
      "Epoch: 25, Loss: 1.19, Accuracy: 0.58\n",
      "Epoch: 26, Loss: 1.13, Accuracy: 0.58\n",
      "Epoch: 27, Loss: 1.10, Accuracy: 0.60\n",
      "Epoch: 28, Loss: 1.09, Accuracy: 0.61\n",
      "Epoch: 29, Loss: 1.03, Accuracy: 0.63\n",
      "Epoch: 30, Loss: 0.99, Accuracy: 0.64\n",
      "Epoch: 31, Loss: 0.96, Accuracy: 0.64\n",
      "Epoch: 32, Loss: 0.89, Accuracy: 0.68\n",
      "Epoch: 33, Loss: 0.91, Accuracy: 0.67\n",
      "Epoch: 34, Loss: 0.85, Accuracy: 0.70\n",
      "Epoch: 35, Loss: 0.81, Accuracy: 0.69\n",
      "Epoch: 36, Loss: 0.77, Accuracy: 0.73\n",
      "Epoch: 37, Loss: 0.82, Accuracy: 0.70\n",
      "Epoch: 38, Loss: 0.77, Accuracy: 0.74\n",
      "Epoch: 39, Loss: 0.75, Accuracy: 0.73\n",
      "Epoch: 40, Loss: 0.72, Accuracy: 0.75\n",
      "Epoch: 41, Loss: 0.68, Accuracy: 0.78\n",
      "Epoch: 42, Loss: 0.66, Accuracy: 0.76\n",
      "Epoch: 43, Loss: 0.66, Accuracy: 0.78\n",
      "Epoch: 44, Loss: 0.67, Accuracy: 0.76\n",
      "Epoch: 45, Loss: 0.65, Accuracy: 0.78\n",
      "Epoch: 46, Loss: 0.62, Accuracy: 0.78\n",
      "Epoch: 47, Loss: 0.54, Accuracy: 0.80\n",
      "Epoch: 48, Loss: 0.54, Accuracy: 0.82\n",
      "Epoch: 49, Loss: 0.52, Accuracy: 0.81\n",
      "Epoch: 50, Loss: 0.45, Accuracy: 0.84\n",
      "Epoch: 51, Loss: 0.50, Accuracy: 0.82\n",
      "Epoch: 52, Loss: 0.46, Accuracy: 0.85\n",
      "Epoch: 53, Loss: 0.41, Accuracy: 0.86\n",
      "Epoch: 54, Loss: 0.42, Accuracy: 0.85\n",
      "Epoch: 55, Loss: 0.40, Accuracy: 0.87\n",
      "Epoch: 56, Loss: 0.39, Accuracy: 0.86\n",
      "Epoch: 57, Loss: 0.42, Accuracy: 0.85\n",
      "Epoch: 58, Loss: 0.37, Accuracy: 0.87\n",
      "Epoch: 59, Loss: 0.36, Accuracy: 0.87\n",
      "Epoch: 60, Loss: 0.34, Accuracy: 0.88\n",
      "Epoch: 61, Loss: 0.31, Accuracy: 0.89\n",
      "Epoch: 62, Loss: 0.31, Accuracy: 0.89\n",
      "Epoch: 63, Loss: 0.30, Accuracy: 0.91\n",
      "Epoch: 64, Loss: 0.27, Accuracy: 0.91\n",
      "Epoch: 65, Loss: 0.28, Accuracy: 0.91\n",
      "Epoch: 66, Loss: 0.27, Accuracy: 0.91\n",
      "Epoch: 67, Loss: 0.27, Accuracy: 0.90\n",
      "Epoch: 68, Loss: 0.27, Accuracy: 0.92\n",
      "Epoch: 69, Loss: 0.25, Accuracy: 0.92\n",
      "Epoch: 70, Loss: 0.23, Accuracy: 0.93\n",
      "Epoch: 71, Loss: 0.24, Accuracy: 0.92\n",
      "Epoch: 72, Loss: 0.23, Accuracy: 0.92\n",
      "Epoch: 73, Loss: 0.21, Accuracy: 0.92\n",
      "Epoch: 74, Loss: 0.17, Accuracy: 0.94\n",
      "Epoch: 75, Loss: 0.20, Accuracy: 0.94\n",
      "Epoch: 76, Loss: 0.18, Accuracy: 0.95\n",
      "Epoch: 77, Loss: 0.14, Accuracy: 0.95\n",
      "Epoch: 78, Loss: 0.18, Accuracy: 0.93\n",
      "Epoch: 79, Loss: 0.14, Accuracy: 0.96\n",
      "Epoch: 80, Loss: 0.16, Accuracy: 0.95\n",
      "Epoch: 81, Loss: 0.16, Accuracy: 0.95\n",
      "Epoch: 82, Loss: 0.14, Accuracy: 0.95\n",
      "Epoch: 83, Loss: 0.13, Accuracy: 0.97\n",
      "Epoch: 84, Loss: 0.12, Accuracy: 0.96\n",
      "Epoch: 85, Loss: 0.15, Accuracy: 0.95\n",
      "Epoch: 86, Loss: 0.14, Accuracy: 0.95\n",
      "Epoch: 87, Loss: 0.13, Accuracy: 0.96\n",
      "Epoch: 88, Loss: 0.14, Accuracy: 0.95\n",
      "Epoch: 89, Loss: 0.13, Accuracy: 0.96\n",
      "Epoch: 90, Loss: 0.14, Accuracy: 0.96\n",
      "Epoch: 91, Loss: 0.13, Accuracy: 0.96\n",
      "Epoch: 92, Loss: 0.12, Accuracy: 0.96\n",
      "Epoch: 93, Loss: 0.12, Accuracy: 0.96\n",
      "Epoch: 94, Loss: 0.10, Accuracy: 0.96\n",
      "Stopping early at epoch 94\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,anneal_strategy='linear')\n",
    "  early_stop_thresh = 10\n",
    "  best_accuracy = -1\n",
    "  best_epoch = -1\n",
    "  map_list = []\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        #print(data)\n",
    "        #inputs, labels = data[0], torch.tensor(data[1])\n",
    "\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        #print(outputs)\n",
    "\n",
    "\n",
    "\n",
    "        labels_tens = torch.tensor(labels)\n",
    "\n",
    "        #print(labels_tens)\n",
    "\n",
    "        loss = criterion(outputs, labels_tens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "      best_accuracy = acc\n",
    "      best_epoch = epoch\n",
    "      checkpoint(myModel, 'best_model_CNN_Conv_attn.pth')\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "      print(f'Stopping early at epoch {epoch}')\n",
    "      #print(map_list)\n",
    "      break\n",
    "    else:\n",
    "      checkpoint(myModel, 'last_model_CNN_Conv_attn.pth')\n",
    "\n",
    "\n",
    "\n",
    "num_epochs=150  # Just for demo, adjust this higher.\n",
    "training(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1699667227901,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "dTedVRSPuUSO"
   },
   "outputs": [],
   "source": [
    "resume(myModel, 'best_model_CNN_Conv_attn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result of CNN+ Convolution Attention on Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1699667231192,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "-h6r1QV8nbOH"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  # Disable gradient updates\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      # Get the input features and target labels, and put them on the GPU\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "      # Normalize the inputs\n",
    "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "      inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "      # Get predictions\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Get the predicted class with the highest score\n",
    "      _, prediction = torch.max(outputs,1)\n",
    "      # Count of predictions that matched the target label\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "\n",
    "  acc = correct_prediction/total_prediction\n",
    "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213438,
     "status": "ok",
     "timestamp": 1699667451448,
     "user": {
      "displayName": "ARKAPRAVO NANDI",
      "userId": "17852750737634586751"
     },
     "user_tz": -660
    },
    "id": "RwHsY_vhndZ7",
    "outputId": "0bb2127a-be0a-4f83-deb8-7b703d81813a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72, Total items: 288\n"
     ]
    }
   ],
   "source": [
    "# Run inference on trained model with the validation set\n",
    "inference(myModel, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
